<div align="center">

# ğŸ”® Turkish Next Word Prediction Engine

### LSTM-Powered Language Model for Turkish Daily Conversations

[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)](https://tensorflow.org)
[![Deep Learning](https://img.shields.io/badge/Deep%20Learning-LSTM-00D4AA?style=for-the-badge&logo=keras&logoColor=white)](https://keras.io)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)

*TÃ¼rkÃ§e gÃ¼nlÃ¼k yaÅŸam cÃ¼mleleri Ã¼zerine eÄŸitilmiÅŸ, derin Ã¶ÄŸrenme tabanlÄ± kelime tahmin motoru*

</div>

---

## ğŸ“Œ Project Motivation

Language modeling, Ã¶zellikle **Next Word Prediction** gÃ¶revi, modern NLP sistemlerinin temel yapÄ± taÅŸlarÄ±ndan biridir. Bu proje:

- **Sentetik veri Ã¼retimi** yoluyla TÃ¼rkÃ§e dil kalÄ±plarÄ±nÄ±n modellenmesini
- **Sequential learning** ile baÄŸlamsal kelime iliÅŸkilerinin Ã¶ÄŸrenilmesini
- **Recurrent Neural Networks** kullanarak uzun dÃ¶nemli baÄŸÄ±mlÄ±lÄ±klarÄ±n yakalanmasÄ±nÄ±

hedeflemektedir. TÃ¼rkÃ§e gibi sondan eklemeli (agglutinative) dillerde kelime tahmini, zengin morfolojik yapÄ± nedeniyle Ã¶zellikle zorlu bir gÃ¶revdir.

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ“Š DATA PIPELINE                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Raw Text â†’ Tokenization â†’ N-gram Sequences â†’ Pre-Padding      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ§  MODEL ARCHITECTURE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input â†’ Embedding(50D) â†’ LSTM(100 units) â†’ Dense(Softmax)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ¯ OUTPUT                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Probability Distribution â†’ argmax â†’ Predicted Word            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technical Deep Dive

| Component | Specification | Purpose |
|-----------|---------------|---------|
| **Embedding Layer** | 50 dimensions | Dense word representations capturing semantic relationships |
| **LSTM Layer** | 100 hidden units | Learning long-term dependencies via gated memory cells |
| **Dense Layer** | Softmax activation | Multi-class probability distribution over vocabulary |
| **Optimizer** | Adam | Adaptive learning rate optimization |
| **Loss Function** | Categorical Crossentropy | Multi-class classification objective |

### N-gram Sequence Generation

```
Input:  "BugÃ¼n hava Ã§ok gÃ¼zel"
        â†“ Tokenization
Tokens: [12, 45, 8, 23]
        â†“ N-gram Generation
Sequences:
  [12, 45]           â†’ Target: 45
  [12, 45, 8]        â†’ Target: 8
  [12, 45, 8, 23]    â†’ Target: 23
        â†“ Pre-Padding
  [0, 0, 12, 45]     â†’ 45
  [0, 12, 45, 8]     â†’ 8
  [12, 45, 8, 23]    â†’ 23
```

---

## âš¡ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ§  **LSTM Power** | Forget/Input/Output gate mekanizmasÄ± ile sequential data'da Ã¼stÃ¼n performans |
| ğŸ‡¹ğŸ‡· **Native Turkish Support** | GÃ¼nlÃ¼k konuÅŸma kalÄ±plarÄ±na uygun Ã¶zel veri seti |
| âš¡ **Scalable Architecture** | Kolayca GRU, Bi-LSTM veya Transformer'a geniÅŸletilebilir |
| ğŸ¯ **Pre-padding Strategy** | DeÄŸiÅŸken uzunluklu sequence'lar iÃ§in tutarlÄ± input shape |
| ğŸ“Š **Softmax Inference** | Vocabulary Ã¼zerinde olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± ile kelime tahmini |

---

## ğŸš€ Installation & Usage

### Prerequisites

```bash
Python >= 3.8
TensorFlow >= 2.x
NumPy
```

### Quick Start

```bash
# Clone the repository
git clone https://github.com/MustafaKocamann/NLP-Projects.git
cd NLP-Projects/text-generation

# Install dependencies
pip install tensorflow numpy

# Train the model and generate text
python train.py
```

### Example Output

```python
# Input seed text
generate_text("BugÃ¼n", 3)

# Possible output
>>> "BugÃ¼n hava Ã§ok gÃ¼zel"
```

---

## ğŸ“ˆ Training Pipeline

```mermaid
graph LR
    A[ğŸ“ Raw Text Data] --> B[ğŸ”¤ Tokenization]
    B --> C[ğŸ“Š N-gram Generation]
    C --> D[ğŸ“ Pre-Padding]
    D --> E[ğŸ§  LSTM Training]
    E --> F[ğŸ’¾ Model Weights]
    F --> G[ğŸ¯ Inference]
```

### Training Configuration

| Parameter | Value |
|-----------|-------|
| Epochs | 100 |
| Embedding Dimension | 50 |
| LSTM Units | 100 |
| Optimizer | Adam |
| Loss | Categorical Crossentropy |

---

## ğŸ”® Future Roadmap

- [ ] ğŸ”„ **GRU Implementation** â€” Daha hÄ±zlÄ± eÄŸitim iÃ§in Gated Recurrent Unit desteÄŸi
- [ ] ğŸ¤– **Transformer Architecture** â€” Self-attention mekanizmasÄ± ile modern dil modelleme
- [ ] ğŸ“š **Large-scale Turkish Corpus** â€” Wikipedia, haber siteleri ve sosyal medya verileri
- [ ] ğŸŒ **Web API** â€” FastAPI ile REST endpoint oluÅŸturma
- [ ] ğŸ“± **Mobile Deployment** â€” TensorFlow Lite ile mobil uygulama entegrasyonu
- [ ] ğŸ¨ **Interactive Demo** â€” Streamlit veya Gradio ile canlÄ± demo arayÃ¼zÃ¼

---

## ğŸ“‚ Project Structure

```
text-generation/
â”œâ”€â”€ train.py          # Main training script with model definition
â”œâ”€â”€ README.md         # Project documentation
â””â”€â”€ requirements.txt  # Python dependencies (optional)
```

---

## ğŸ¤ Contributing

Contributions are welcome! Feel free to:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<div align="center">

**Built with â¤ï¸ for the Turkish NLP Community**

[![GitHub](https://img.shields.io/badge/GitHub-MustafaKocamann-181717?style=flat-square&logo=github)](https://github.com/MustafaKocamann)

</div>
